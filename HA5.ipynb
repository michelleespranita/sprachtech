{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mayra Diandra Nabila Ratnadi (3039839)\n",
    "\n",
    "Michelle Espranita Liman (3072994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2019/2020\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Word Sense Induction\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Thursday, 21.11.2019 (23:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn  \n",
    "from nltk.book import text2,text3 \n",
    "from nltk.text import Text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.cluster import KMeans  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize affection_contexts_no_stopwords so that it is the same like the one in the exercise\n",
    "\n",
    "c = nltk.ConcordanceIndex(text2.tokens, key=lambda s: s.lower())  \n",
    "window_size = 10  \n",
    "affection_contexts = []  \n",
    "for index in c.offsets('affection'):  #c.tokens()[index] --> 'affection'\n",
    "    if index > 10:  # Because the indexes 0-10 are just title\n",
    "        affection_contexts.append(text2.tokens[index - window_size:index + window_size])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))  \n",
    "# Homework = []  \n",
    "for context in affection_contexts: \n",
    "    # remove punctuation \n",
    "    context = ' '.join(context).translate(str.maketrans('', '', string.punctuation))\n",
    "    context = context.replace('  ', ' ') \n",
    "    # remove stopwords  \n",
    "    context_no_stopwords = []  \n",
    "    for token in context.split(' '):  \n",
    "        if token not in stopwords:  \n",
    "            context_no_stopwords.append(token)  \n",
    "            affection_contexts_no_stopwords.append(' '.join(context_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.1.1.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "Try K-means clustering with genism word2vec embeddings features instead of tf-idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=5, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec #get gensim word2vec feature extraction utility  \n",
    "affection_contexts_no_stopwords = [context.split(' ') for context in affection_contexts_no_stopwords]  \n",
    "\n",
    "# Apply word2vec algorithm\n",
    "\n",
    "w2v = Word2Vec(affection_contexts_no_stopwords, size=1000, window=11, min_count=3)\n",
    "\n",
    "X_w2v = w2v[w2v.wv.vocab]  \n",
    "\n",
    "\n",
    "model_w2v = KMeans(n_clusters=true_k) \n",
    "\n",
    "# Train kmeans algorithm\n",
    "model_w2v.fit(X_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.1.2.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "Try K-means clustering with genism word2vec embeddings features instead of tf-idf features. Evaluate the performance of the k-means algorithm using silhouette metric for embedding and tf-idf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affection_contexts_no_stopwords has to be remade, because it was already overwritten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nltk.ConcordanceIndex(text2.tokens, key=lambda s: s.lower())  \n",
    "window_size = 10  \n",
    "affection_contexts = []  \n",
    "for index in c.offsets('affection'):  #c.tokens()[index] --> 'affection'\n",
    "    if index > 10:  # Because the indexes 0-10 are just title\n",
    "        affection_contexts.append(text2.tokens[index - window_size:index + window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))  \n",
    "affection_contexts_no_stopwords = []  \n",
    "for context in affection_contexts: \n",
    "    # remove punctuation \n",
    "    context = ' '.join(context).translate(str.maketrans('', '', string.punctuation))\n",
    "    context = context.replace('  ', ' ') \n",
    "    # remove stopwords  \n",
    "    context_no_stopwords = []  \n",
    "    for token in context.split(' '):  \n",
    "        if token not in stopwords:  \n",
    "            context_no_stopwords.append(token)  \n",
    "            affection_contexts_no_stopwords.append(' '.join(context_no_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()  \n",
    "X_tf_idf = vectorizer.fit_transform(affection_contexts_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 3 # number of clusters  \n",
    "model_tf_idf = KMeans(n_clusters=true_k) # apply algorithm  \n",
    "model_tf_idf.fit(X_tf_idf) # training the algorithm\n",
    "# each row of X_tf_idf is a training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score  \n",
    "\n",
    "model_w2v = KMeans(n_clusters=true_k)  \n",
    "model_w2v.fit(X_w2v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63331354\n",
      "0.024039653042394298\n"
     ]
    }
   ],
   "source": [
    "# print Silhouette scores\n",
    "\n",
    "print(silhouette_score(X_w2v, model_w2v.labels_)) # embedding\n",
    "print(silhouette_score(X_tf_idf, model_tf_idf.labels_)) # tf-idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
