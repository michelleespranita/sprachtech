{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mayra Diandra Nabila Ratnadi (3039839)\n",
    "\n",
    "Michelle Espranita Liman (3072994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we assumed that the spelling corrector is implemented for writing mistakes, since the test set given is a handwritten essay. Thus, we take no typing mistakes into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import bigrams\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigramFDist():\n",
    "    with open(\"deu_news_2015_30K-sentences.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = f.read()\n",
    "    corpus = re.findall(r'\\w+', corpus)\n",
    "    corpus = [word for word in corpus if not any([letter in ['0','1','2','3','4','5','6','7','8','9'] for letter in word])]\n",
    "    # corpus is a list of words\n",
    "    fdist = FreqDist(corpus)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramFDist():\n",
    "    with open(\"deu_news_2015_30K-sentences.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = f.read().splitlines()\n",
    "    sentences = [word_tokenize(sentence) for sentence in corpus]\n",
    "    sentences = [sentence[2:] for sentence in sentences]\n",
    "    bi = [bigrams(sentence) for sentence in sentences]\n",
    "    bi = [bigram for sublist in bi for bigram in sublist]\n",
    "    bi_fdist = FreqDist(bi)\n",
    "    return bi_fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist = unigramFDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_fdist = bigramFDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellingErrors = pd.read_csv(\"spellingErrors.csv\", sep='\\t', names=['wrong', 'right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneEditAway(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyzäüöß'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    capitalize = [word.capitalize()]\n",
    "    return set(deletes + transposes + replaces + inserts + capitalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoEditsAway(word):\n",
    "    return set(e2 for e1 in oneEditAway(word) for e2 in oneEditAway(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates(word):\n",
    "    return oneEditAway(word) | twoEditsAway(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knownCandidates(word):\n",
    "    cands = candidates(word)\n",
    "    return [cand for cand in cands if fdist[cand]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cognitiveCandidates(word):\n",
    "    cands = []\n",
    "    for char in word:\n",
    "        if char in list(spellingErrors[\"wrong\"]):\n",
    "            replacement = spellingErrors.loc[spellingErrors['wrong']==char]['right'].values[0]\n",
    "            idx = word.find(char)\n",
    "            cands.append(word[:idx] + replacement + word[(idx+1):])\n",
    "    for i in range(len(word)-1):\n",
    "        letters = word[i] + word[i+1]\n",
    "        if letters in list(spellingErrors[\"wrong\"]):\n",
    "            replacement = spellingErrors.loc[spellingErrors['wrong']==letters]['right'].values[0]\n",
    "            idx = word.find(letters)\n",
    "            cands.append(word[:idx] + replacement + word[(idx+2):])\n",
    "    return set(cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cognitiveCandidates2(word):\n",
    "    return set(c2 for c1 in cognitiveCandidates(word) for c2 in cognitiveCandidates(c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalCognitiveCandidates(word):\n",
    "    return list(cognitiveCandidates(word) | cognitiveCandidates2(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hCandidates(word):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    splits = [(L, R) for (L, R) in splits if len(L)!=0]\n",
    "    hCands = [L + 'h' + R for (L,R) in splits]\n",
    "    scores = {cand: fdist.freq(cand) for cand in hCands if fdist.freq(cand) > 0}\n",
    "    filteredHCands = list(scores.keys())\n",
    "    return filteredHCands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatedCandidates(word):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    splits = [(L, R) for (L, R) in splits if len(L)!=0]\n",
    "    repeatedCands = [L + L[-1] + R for (L,R) in splits]\n",
    "    scores = {cand: fdist.freq(cand) for cand in repeatedCands if fdist.freq(cand) > 0}\n",
    "    filteredRepeatedCands = list(scores.keys())\n",
    "    return filteredRepeatedCands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingCandidates(word):\n",
    "    missingCands = []\n",
    "    for char in word:\n",
    "        idx = word.find(char)\n",
    "        missingCands.append(word[:idx] + word[(idx+1):])\n",
    "    missingCands = list(set(missingCands))\n",
    "    scores = {cand: fdist.freq(cand) for cand in missingCands if fdist.freq(cand) > 0}\n",
    "    filteredMissingCands = list(scores.keys())\n",
    "    return filteredMissingCands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(seq1, seq2):\n",
    "    \n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    \n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    \n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "    \n",
    "        \n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            \n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                cost = 0\n",
    "            \n",
    "            ### cost for only changing letter case is set to 0.5\n",
    "            elif seq1[x-1].lower() == seq2[y-1].lower():\n",
    "                cost = 0.5\n",
    "            \n",
    "            else:\n",
    "                cost = 1\n",
    "                \n",
    "            matrix [x,y] = min(\n",
    "                matrix[x-1,y] + 1,\n",
    "                matrix[x-1,y-1] + cost,\n",
    "                matrix[x,y-1] + 1\n",
    "                )\n",
    "            \n",
    "    #print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word, prev):\n",
    "    if fdist.freq(word)>0 or word in string.punctuation: # If it exists in the dataset, most probably it is already correct (no need to be corrected)\n",
    "        return word\n",
    "    else:\n",
    "        # 1: Missing letters that are repeated (This is more probable, so we will test this first)\n",
    "        repeatedCands = repeatedCandidates(word)\n",
    "        if len(repeatedCands) == 1:\n",
    "            return repeatedCands[0]\n",
    "        # 2: Too many letters are repeated\n",
    "        missingCands = missingCandidates(word)\n",
    "        if len(missingCands) == 1:\n",
    "            return missingCands[0]\n",
    "        # 3: Missing 'h's (Because they are often silent in German)\n",
    "        hCands = hCandidates(word)\n",
    "        if len(hCands) == 1:\n",
    "            return hCands[0]\n",
    "        # 4: Capitalization error\n",
    "        if fdist.freq(word.capitalize())>0:\n",
    "            return word.capitalize()\n",
    "        # 5: Pronounciation (Cognitive) error and Real Word error\n",
    "        knownCands = knownCandidates(word)\n",
    "        cognitiveCands = totalCognitiveCandidates(word)\n",
    "        allCands = knownCands + cognitiveCands\n",
    "        scores = {cand: ((len(word) - levenshtein(word, cand))/len(word)) * fdist.freq(cand) for cand in allCands}\n",
    "        if all([score==0.0 for (word, score) in list(scores.items())]): # If all the candidates cannot be found in the dataset (Dataset insufficient/The error is too much)\n",
    "            return word\n",
    "        else: # Consider bigrams with the word before to bring some context, thus better judgment\n",
    "            fiveMostLikelyScore = Counter(scores).most_common(5)\n",
    "            fiveMostLikely = [scorer for (scorer, score) in fiveMostLikelyScore]\n",
    "            bigrams = [(prev, item) for item in fiveMostLikely]\n",
    "            scores = {big: bi_fdist.freq(big) + score for big in bigrams for (scorer, score) in fiveMostLikelyScore if big[1] == scorer}\n",
    "            return Counter(scores).most_common(1)[0][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctText(listOfTokens):\n",
    "    fdist = unigramFDist()\n",
    "    corrections = []\n",
    "    listOfTokens.insert(0, '')\n",
    "    for i in range(1, len(listOfTokens)):\n",
    "        prev = listOfTokens[i-1]\n",
    "        word = listOfTokens[i]\n",
    "        if word == '':\n",
    "            continue\n",
    "        print(\"Original: \", word)\n",
    "        correctWord = correction(word, prev)\n",
    "        listOfTokens[i] = correctWord\n",
    "        print(\"Corrected: \", correctWord)\n",
    "        corrections.append(correctWord)\n",
    "    listOfTokens.pop(0)\n",
    "    return corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(corrected_tokens, original_spellings, solution):\n",
    "    \n",
    "    #make sure that all three lists have the same length\n",
    "    assert len(corrected_tokens) == len(original_spellings) == len(solution), \"Error. The lists with proposed corrections, original tokens and the solution do not have the same lengths.\"\n",
    "    \n",
    "    #count number of incorrect tokens (=whenever there is a difference between the proposed correction and the gold solution)\n",
    "    incorrect_tokens = sum(1 for i in range(len(corrected_tokens)) if corrected_tokens[i] != solution[i])\n",
    "    \n",
    "    print(\"Number of incorrect tokens in the text:\", incorrect_tokens)\n",
    "    \n",
    "\n",
    "def correct_and_evaluate(filename):\n",
    "    \n",
    "    #read in text\n",
    "    with open(filename, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().splitlines()\n",
    "    text = [line.split(\"\\t\") for line in text]\n",
    "\n",
    "    #extract original spelling and solution for every word\n",
    "    original_spellings = [line[0] for line in text]\n",
    "    solution = [line[1] for line in text]\n",
    "\n",
    "    #correct tokens\n",
    "    proposed_corrections = correctText(original_spellings)\n",
    "    \n",
    "    #evaluate corrections\n",
    "    evaluate(proposed_corrections, original_spellings, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Lars\n",
      "Corrected:  Lars\n",
      "Original:  und\n",
      "Corrected:  und\n",
      "Original:  Lea\n",
      "Corrected:  Lea\n",
      "Original:  kaufen\n",
      "Corrected:  kaufen\n",
      "Original:  ein\n",
      "Corrected:  ein\n",
      "Original:  eis\n",
      "Corrected:  Eis\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Original:  Der\n",
      "Corrected:  Der\n",
      "Original:  eisferkeufer\n",
      "Corrected:  eisferkeufer\n",
      "Original:  gibt\n",
      "Corrected:  gibt\n",
      "Original:  in\n",
      "Corrected:  in\n",
      "Original:  eins\n",
      "Corrected:  eins\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Original:  Dan\n",
      "Corrected:  Dan\n",
      "Original:  gen\n",
      "Corrected:  gen\n",
      "Original:  sie\n",
      "Corrected:  sie\n",
      "Original:  und\n",
      "Corrected:  und\n",
      "Original:  esen\n",
      "Corrected:  essen\n",
      "Original:  ir\n",
      "Corrected:  ihr\n",
      "Original:  eis\n",
      "Corrected:  Eis\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Original:  Dodo\n",
      "Corrected:  Doch\n",
      "Original:  fenkt\n",
      "Corrected:  fest\n",
      "Original:  die\n",
      "Corrected:  die\n",
      "Original:  tropfen\n",
      "Corrected:  Tropfen\n",
      "Original:  die\n",
      "Corrected:  die\n",
      "Original:  runter\n",
      "Corrected:  runter\n",
      "Original:  fallen\n",
      "Corrected:  fallen\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Original:  Dodo\n",
      "Corrected:  Doch\n",
      "Original:  get\n",
      "Corrected:  geht\n",
      "Original:  so\n",
      "Corrected:  so\n",
      "Original:  langsam\n",
      "Corrected:  langsam\n",
      "Original:  Lea\n",
      "Corrected:  Lea\n",
      "Original:  Stolpat\n",
      "Corrected:  Soldat\n",
      "Original:  üba\n",
      "Corrected:  über\n",
      "Original:  dodo\n",
      "Corrected:  doch\n",
      "Original:  und\n",
      "Corrected:  und\n",
      "Original:  das\n",
      "Corrected:  das\n",
      "Original:  eis\n",
      "Corrected:  Eis\n",
      "Original:  fellt\n",
      "Corrected:  stellt\n",
      "Original:  ir\n",
      "Corrected:  ihr\n",
      "Original:  aus\n",
      "Corrected:  aus\n",
      "Original:  der\n",
      "Corrected:  der\n",
      "Original:  Hand\n",
      "Corrected:  Hand\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Original:  Sie\n",
      "Corrected:  Sie\n",
      "Original:  fellt\n",
      "Corrected:  stellt\n",
      "Original:  hinn\n",
      "Corrected:  hin\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Original:  Lars\n",
      "Corrected:  Lars\n",
      "Original:  hilft\n",
      "Corrected:  hilft\n",
      "Original:  ir\n",
      "Corrected:  ihr\n",
      "Original:  und\n",
      "Corrected:  und\n",
      "Original:  dodo\n",
      "Corrected:  doch\n",
      "Original:  lekt\n",
      "Corrected:  seit\n",
      "Original:  das\n",
      "Corrected:  das\n",
      "Original:  eis\n",
      "Corrected:  Eis\n",
      "Original:  auf\n",
      "Corrected:  auf\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Original:  Lars\n",
      "Corrected:  Lars\n",
      "Original:  hat\n",
      "Corrected:  hat\n",
      "Original:  eine\n",
      "Corrected:  eine\n",
      "Original:  ide\n",
      "Corrected:  der\n",
      "Original:  er\n",
      "Corrected:  er\n",
      "Original:  giebt\n",
      "Corrected:  gibt\n",
      "Original:  eine\n",
      "Corrected:  eine\n",
      "Original:  kugel\n",
      "Corrected:  Kugel\n",
      "Original:  eis\n",
      "Corrected:  Eis\n",
      "Original:  Lea\n",
      "Corrected:  Lea\n",
      "Original:  dan\n",
      "Corrected:  dann\n",
      "Original:  gen\n",
      "Corrected:  gen\n",
      "Original:  sie\n",
      "Corrected:  sie\n",
      "Original:  glüklieg\n",
      "Corrected:  glücklich\n",
      "Original:  .\n",
      "Corrected:  .\n",
      "Number of incorrect tokens in the text: 15\n"
     ]
    }
   ],
   "source": [
    "correct_and_evaluate(\"spelling_homework.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
