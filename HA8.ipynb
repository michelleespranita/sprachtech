{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Style des gesamten Dokuments */\n",
       "#notebook-container {\n",
       "\tfont-family: \"NimbusMonL-ReguObli\";\n",
       "\tfont-size: 120%\n",
       "}\n",
       "\n",
       "/* Style f√ºr die √úberschrift: Zentriert diese und stellt sie fett dar. */\n",
       ".headline {\n",
       "\ttext-align: center;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 185.7%\n",
       "}\n",
       "\n",
       "/* Style f√ºr die Aufgabenbeschreibung. Z.B.: \"√úbung zum Thema...\" */\n",
       ".description {\n",
       "\ttext-align: center;\n",
       "\tfont-size: 145.7%\n",
       "}\n",
       "\n",
       "/* Hebt das Abgabedatum fett und kursiv hervor */\n",
       "#submission {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "/* Style f√ºr das eigentliche Thema. Z.B.: \"Intelligenz\" */\n",
       "#topic {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       ".task_description {\n",
       "\tmargin-bottom: 20px;\n",
       "}\n",
       "\n",
       "/* Hebt die Aufgabennummerierung fett hervor. */\n",
       ".task {\n",
       "\tfont-style: normal;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 120%;\n",
       "\tborder-bottom: 2px solid black;\n",
       "  background-color: #97CAEF;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 50px;\n",
       "  padding-right: 50px;\n",
       "}\n",
       "\n",
       ".subtask {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #CAFAFE;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 25px;\n",
       "  padding-right: 25px;\n",
       "}\n",
       "\n",
       ".l1 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #14A76C;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l2 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FFE400;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l3 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FF652F;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".points {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       "ol.lower_roman {\n",
       "    list-style-type: lower-roman;\n",
       "}\n",
       "\n",
       "ol.characters {\n",
       "    list-style-type: lower-alpha;\n",
       "}\n",
       "\n",
       "/* Style einer Code-Cell */\n",
       ".CodeMirror-code {\n",
       "\tbackground-color: #ededed\n",
       "}\n",
       "\n",
       "/* Style eines Kommentars im Code √§ndern. */\n",
       ".cm-s-ipython span.cm-comment {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-atom {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-number {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Keywords √§ndern */\n",
       ".cm-s-ipython span.cm-keyword {\n",
       "\tcolor: #B000B0\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-def {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Python-Variable √§ndern */\n",
       ".cm-s-ipython span.cm-variable {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Property √§ndern */\n",
       ".cm-s-ipython span.cm-property {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Operators √§ndern */\n",
       ".cm-s-ipython span.cm-operator {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Strings √§ndern */\n",
       ".cm-s-ipython span.cm-string {\n",
       "\tcolor: brown;\n",
       "}\n",
       "\n",
       "/* Style einer eingebauten Funktion √§ndern (z.B. \"open\") */\n",
       ".cm-s-ipython span.cm-builtin {\n",
       "\n",
       "}\n",
       "\n",
       "/* Hebt hervor, welche Klammern zueinander passen */\n",
       ".cm-s-ipython .CodeMirror-matchingbracket {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-variable-2 {\n",
       "\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mayra Diandra Nabila Ratnadi (3039839)\n",
    "\n",
    "Michelle Espranita Liman (3072994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2019/2020\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    √úbung zum Thema <i id=\"topic\">\"Keyphrase Extraction\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Thursday, 12.12.2019 (23:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import movie_reviews\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">8.1.</i> :::10 Homework points:::\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design an algorithm to find the \"statistically improbable phrases\" (SIP) of a document collection:<br><br>\n",
    "https://web.archive.org/web/20160425062531/http://www.amazon.com/gp/search-inside/sipshelp.html <br><br>\n",
    "The main idea behind SIPs is that if a phrase occurs much more often in a document than this is statistically expected (hence the name ‚Äústatistically improbable phrases‚Äù), it might be a good indicator of what a document is about.<br><br> For example, we may look at a corpus with 1,000,000 words and see that the word ‚Äúcar‚Äù appears 20 times. If now the word ‚Äúcar‚Äù also appears 20 times in a document with 1000 words, this seems improbable.<br><br>\n",
    "To formalize the counting of words a bit, we may use so called contingency tables. They are a structured way to display how often a word occurs in certain documents. <br> As we are only interested in the counts in the current document as well as the counts in the overall collection, we will use a 2x2 table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|-|Document<br>(Foreground Corpus)|Other Corpus<br>(Background Corpus)|-|\n",
    "|-|-|-|-|\n",
    "|word|A|B|-\n",
    "|all other words|C-A|D-B|-\n",
    "|-|C|D|N = C + D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now measure how ‚Äúimprobable‚Äù is a certain phrase in a document (also called foreground corpus) w.r.t. some corpus (called background corpus) using ‚ÄúLog-Likelyhood‚Äù (LL) as defined by (Rayson & Garside, 2000). <br><br>In terms of the contingency table, it is defined as:<br><br>\n",
    "\n",
    "$ LL = 2*(A*log_2(\\frac{A}{E_1}) + B*log_2(\\frac{B}{E_2}))$ <br><br> where <br><br>\n",
    "$E_1 = \\frac{C*(A+B)}{N}$ and $E_2 = \\frac{D*(A+B)}{N}$ <br><br> For simplifications, you may limit phrases to bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.1.</i>\n",
    "</div>\n",
    "\n",
    "Select a suitable corpus from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "text2 = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.2.</i>\n",
    "</div>\n",
    "\n",
    "Initialize two frequency distributions of bigrams. <br>One from a text of your choice (`fdist_fg`) and one of the\n",
    "corpus selected in 8.1.1. (`fdist_bg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 201 samples and 232 outcomes>\n",
      "('Amazon.com', 'Statistically') 1\n",
      "('Statistically', 'Improbable') 2\n",
      "('Improbable', 'Phrases') 2\n",
      "('Phrases', '.') 1\n",
      "('.', 'Amazon.com') 1\n",
      "('Amazon.com', \"'s\") 1\n",
      "(\"'s\", 'Statistically') 1\n",
      "('Phrases', ',') 1\n",
      "(',', 'or') 1\n",
      "('or', \"'SIPs\") 1\n",
      "(\"'SIPs\", \"'\") 1\n",
      "(\"'\", ',') 1\n",
      "(',', 'are') 1\n",
      "('are', 'the') 1\n",
      "('the', 'most') 1\n",
      "('most', 'distinctive') 1\n",
      "('distinctive', 'phrases') 1\n",
      "('phrases', 'in') 1\n",
      "('in', 'the') 3\n",
      "('the', 'text') 2\n",
      "('text', 'of') 2\n",
      "('of', 'books') 2\n",
      "('books', 'in') 4\n",
      "('the', 'Search') 2\n",
      "('Search', 'Inside') 4\n",
      "('Inside', '!') 4\n",
      "('!', '‚Ñ¢') 1\n",
      "('‚Ñ¢', 'program') 1\n",
      "('program', '.') 2\n",
      "('.', 'To') 1\n",
      "('To', 'identify') 1\n",
      "('identify', 'SIPs') 1\n",
      "('SIPs', ',') 1\n",
      "(',', 'our') 1\n",
      "('our', 'computers') 1\n",
      "('computers', 'scan') 1\n",
      "('scan', 'the') 1\n",
      "('of', 'all') 1\n",
      "('all', 'books') 2\n",
      "('!', 'program') 1\n",
      "('.', 'If') 1\n",
      "('If', 'they') 1\n",
      "('they', 'find') 1\n",
      "('find', 'a') 1\n",
      "('a', 'phrase') 1\n",
      "('phrase', 'that') 1\n",
      "('that', 'occurs') 1\n",
      "('occurs', 'a') 1\n",
      "('a', 'large') 1\n",
      "('large', 'number') 1\n",
      "('number', 'of') 1\n",
      "('of', 'times') 1\n",
      "('times', 'in') 1\n",
      "('in', 'a') 1\n",
      "('a', 'particular') 2\n",
      "('particular', 'book') 2\n",
      "('book', 'relative') 1\n",
      "('relative', 'to') 2\n",
      "('to', 'all') 2\n",
      "('all', 'Search') 1\n",
      "('!', 'books') 1\n",
      "('books', ',') 1\n",
      "(',', 'that') 1\n",
      "('that', 'phrase') 1\n",
      "('phrase', 'is') 1\n",
      "('is', 'a') 1\n",
      "('a', 'SIP') 2\n",
      "('SIP', 'in') 1\n",
      "('in', 'that') 1\n",
      "('that', 'book') 1\n",
      "('book', '.') 2\n",
      "('.', 'SIPs') 1\n",
      "('SIPs', 'are') 1\n",
      "('are', 'not') 1\n",
      "('not', 'necessarily') 1\n",
      "('necessarily', 'improbable') 1\n",
      "('improbable', 'within') 1\n",
      "('within', 'a') 1\n",
      "('book', ',') 1\n",
      "(',', 'but') 1\n",
      "('but', 'they') 1\n",
      "('they', 'are') 1\n",
      "('are', 'improbable') 1\n",
      "('improbable', 'relative') 1\n",
      "('in', 'Search') 1\n",
      "('!', '.') 1\n",
      "('.', 'For') 2\n",
      "('For', 'example') 1\n",
      "('example', ',') 1\n",
      "(',', 'most') 1\n",
      "('most', 'SIPs') 1\n",
      "('SIPs', 'for') 1\n",
      "('for', 'a') 1\n",
      "('a', 'book') 1\n",
      "('book', 'on') 1\n",
      "('on', 'taxes') 1\n",
      "('taxes', 'are') 1\n",
      "('are', 'tax') 1\n",
      "('tax', 'related') 1\n",
      "('related', '.') 1\n",
      "('.', 'But') 1\n",
      "('But', 'because') 1\n",
      "('because', 'we') 1\n",
      "('we', 'display') 1\n",
      "('display', 'SIPs') 1\n",
      "('SIPs', 'in') 1\n",
      "('in', 'order') 1\n",
      "('order', 'of') 1\n",
      "('of', 'their') 1\n",
      "('their', 'improbability') 1\n",
      "('improbability', 'score') 1\n",
      "('score', ',') 1\n",
      "(',', 'the') 1\n",
      "('the', 'first') 1\n",
      "('first', 'SIPs') 1\n",
      "('SIPs', 'will') 1\n",
      "('will', 'be') 1\n",
      "('be', 'on') 1\n",
      "('on', 'tax') 1\n",
      "('tax', 'topics') 1\n",
      "('topics', 'that') 1\n",
      "('that', 'this') 1\n",
      "('this', 'book') 1\n",
      "('book', 'mentions') 1\n",
      "('mentions', 'more') 1\n",
      "('more', 'often') 1\n",
      "('often', 'than') 1\n",
      "('than', 'other') 1\n",
      "('other', 'tax') 1\n",
      "('tax', 'books') 1\n",
      "('books', '.') 1\n",
      "('For', 'works') 1\n",
      "('works', 'of') 1\n",
      "('of', 'fiction') 1\n",
      "('fiction', ',') 1\n",
      "(',', 'SIPs') 1\n",
      "('SIPs', 'tend') 1\n",
      "('tend', 'to') 1\n",
      "('to', 'be') 1\n",
      "('be', 'distinctive') 1\n",
      "('distinctive', 'word') 1\n",
      "('word', 'combinations') 1\n",
      "('combinations', 'that') 1\n",
      "('that', 'often') 1\n",
      "('often', 'hint') 1\n",
      "('hint', 'at') 1\n",
      "('at', 'important') 1\n",
      "('important', 'plot') 1\n",
      "('plot', 'elements') 1\n",
      "('elements', '.') 1\n",
      "('.', 'Click') 1\n",
      "('Click', 'on') 1\n",
      "('on', 'a') 1\n",
      "('SIP', 'to') 1\n",
      "('to', 'view') 1\n",
      "('view', 'a') 2\n",
      "('a', 'list') 2\n",
      "('list', 'of') 2\n",
      "('in', 'which') 1\n",
      "('which', 'the') 1\n",
      "('the', 'phrase') 3\n",
      "('phrase', 'occurs') 1\n",
      "('occurs', '.') 1\n",
      "('.', 'You') 1\n",
      "('You', 'can') 1\n",
      "('can', 'also') 1\n",
      "('also', 'view') 1\n",
      "('of', 'references') 1\n",
      "('references', 'to') 1\n",
      "('to', 'the') 1\n",
      "('phrase', 'in') 1\n",
      "('in', 'each') 1\n",
      "('each', 'book') 1\n",
      "('.', 'Learn') 1\n",
      "('Learn', 'more') 1\n",
      "('more', 'about') 1\n",
      "('about', 'the') 1\n",
      "('phrase', 'by') 1\n",
      "('by', 'clicking') 1\n",
      "('clicking', 'on') 1\n",
      "('on', 'the') 1\n",
      "('the', 'A9.com') 1\n",
      "('A9.com', 'search') 1\n",
      "('search', 'link') 1\n",
      "('link', '.') 1\n",
      "('.', 'Have') 1\n",
      "('Have', 'some') 1\n",
      "('some', 'ideas') 1\n",
      "('ideas', 'for') 1\n",
      "('for', 'improving') 1\n",
      "('improving', 'this') 1\n",
      "('this', 'feature') 1\n",
      "('feature', '?') 1\n",
      "('?', 'Please') 1\n",
      "('Please', 'send') 1\n",
      "('send', 'your') 1\n",
      "('your', 'feedback') 1\n",
      "('feedback', 'to') 1\n",
      "('to', 'sitb-feedback') 1\n",
      "('sitb-feedback', '@') 1\n",
      "('@', 'amazon.com') 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4ec65ba3b966>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtokens2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mbigram2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mfdist_fg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m    105\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \"\"\"\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1362\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "text1 = \"Amazon.com Statistically Improbable Phrases. Amazon.com's Statistically Improbable Phrases, or 'SIPs', are the most distinctive phrases in the text of books in the Search Inside!‚Ñ¢ program. To identify SIPs, our computers scan the text of all books in the Search Inside! program. If they find a phrase that occurs a large number of times in a particular book relative to all Search Inside! books, that phrase is a SIP in that book. SIPs are not necessarily improbable within a particular book, but they are improbable relative to all books in Search Inside!. For example, most SIPs for a book on taxes are tax related. But because we display SIPs in order of their improbability score, the first SIPs will be on tax topics that this book mentions more often than other tax books. For works of fiction, SIPs tend to be distinctive word combinations that often hint at important plot elements. Click on a SIP to view a list of books in which the phrase occurs. You can also view a list of references to the phrase in each book. Learn more about the phrase by clicking on the A9.com search link. Have some ideas for improving this feature? Please send your feedback to sitb-feedback@amazon.com\"\n",
    "tokens1 = nltk.word_tokenize(text1)\n",
    "bigram1 = nltk.bigrams(tokens1)\n",
    "fdist_bg = nltk.FreqDist(bigram1)\n",
    "print(fdist_bg)\n",
    "for v,k in fdist_bg.items():\n",
    "    print(v,k)\n",
    "\n",
    "tokens2 = nltk.word_tokenize(text2)\n",
    "bigram2 = nltk.bigrams(tokens2)\n",
    "fdist_fg = nltk.FreqDist(bigram2)\n",
    "print(fdist_fg)\n",
    "for v,k in fdist_fg.items():\n",
    "    print(v,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.3.</i>\n",
    "</div>\n",
    "\n",
    "Using the frequency distributions, determine how to get all the required counts to compute the log likelihood\n",
    "scores and write a function ``compute_LL(phrase,fdist_fg,fdist_bg).``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_LL(phrase,fdist_fg,fdist_bg):\n",
    "    words = nltk.word_tokenize(phrase)\n",
    "    w1 = count(words[0])\n",
    "    w2 = count(words[1])\n",
    "    return 2‚àó(w1‚àómath.log2(fdist_fg)+w2‚àómath.ùëôùëúùëî2(fdist_bg)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.4.</i>\n",
    "</div>\n",
    "\n",
    "Given a text, calculate the LL score for each bigram and output the 10 most ‚Äúimprobable‚Äù phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
