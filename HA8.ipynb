{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'style.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-f4f5e3356ef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"<style>\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"style.css\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"</style>\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'style.css'"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mayra Diandra Nabila Ratnadi (3039839)\n",
    "\n",
    "Michelle Espranita Liman (3072994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2019/2020\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Keyphrase Extraction\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Thursday, 12.12.2019 (23:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import movie_reviews\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.1:</i> <br>\n",
    "</div>\n",
    "\n",
    "Stopword: Which of the following statements are true?\n",
    "1. A stopword is a word used to stop a parsing process.\n",
    "2. A stopword is a high-frequency word.\n",
    "3. Punctuation marks like “.”, “,”, and “;” are stop words.\n",
    "4. “the, to, and” are stop words.\n",
    "5. Stopwords have a maximum length of 3.\n",
    "6. NLTK offers a list of English stopwords through:\n",
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. is false. A stopword is a frequent word with little lexical content.\n",
    "2. is true. Additionally, stopwords (‘a’, ‘the’, ‘to’, ‘also’) must also carry little lexical content.\n",
    "3. is false. First, punctuation marks are not really “words”, and are thus usually not considered stop words. In a sense, they are similar to stop words as they carry no lexical content, i.e. they have no real meaning.\n",
    "4. is true\n",
    "5. is false. There are stopwords like ‘while’ or ‘will’ with length more than 3 characters;\n",
    "6. is true. There are 127 words in this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.2:</i> <br>\n",
    "</div>\n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "1. Words like \"the\", \"and\", \"but\" have high TF-IDF values.\n",
    "2. The foreground corpus can consist of just one document.\n",
    "3. Keyphrase extraction works only if we have enough training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. False. These words are stopwords and can be expected to appear in almost every document. Therefore, their TF-IDF values will be low.\n",
    "\n",
    "2. True. Often the foreground corpus is just a single document from which we want to extract keyphrases, but it can also be a collection of documents.\n",
    "\n",
    "3. False. There are several methods (like the language model approach or TF-IDF) which can be sucessfully used for keyphrase extraction, which do not require any manually labeled training data as input. However, what we do often need is a large background corpus to compare word frequencies with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.3:</i> <br>\n",
    "</div> \n",
    "\n",
    "Search the Web using Google for “powerful tea” (inside quotes) and “strong tea”. Discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>\n",
    "\n",
    "“strong tea” - 265.000.000  hits “powerful tea” - 166.000.000 hits\n",
    "You may get different numbers as the index used by the search engines is never the same. However, the very different counts indicate that “strong tea” is a collocation in English, while “powerful tea” is not. Even if “strong” and “powerful” are considered to be synonyms in many contexts, they cannot be interchanged here.\n",
    "Note that using Google for deriving such hits is considered bad science (as it is not reproducible, it is unclear which texts the counts are based on, etc.). It is better to use a fixed corpus. However, using very large corpora (as big as the Web) to obtain reliable counts is non-trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.4:</i> <br>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.4.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Try to explain the function below! What is done in each line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd1 = nltk.FreqDist(text1)\n",
    "fd1.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "* initialize FD, pass the name of the text as an argument <br>\n",
    "* print five most common types <br>\n",
    "<br>\n",
    "The function extracts the word with the highest frequency out of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.4.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Initialize a frequency distribution with a corpus of your choice, then extract the keywords of the corpus (what is it about), by writing a function that retrieves the 20 most frequent words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9397),\n",
       " ('to', 4063),\n",
       " ('.', 3975),\n",
       " ('the', 3861),\n",
       " ('of', 3565),\n",
       " ('and', 3350),\n",
       " ('her', 2436),\n",
       " ('a', 2043),\n",
       " ('I', 2004),\n",
       " ('in', 1904),\n",
       " ('was', 1846),\n",
       " ('it', 1568),\n",
       " ('\"', 1506),\n",
       " (';', 1419),\n",
       " ('she', 1333),\n",
       " ('be', 1305),\n",
       " ('that', 1297),\n",
       " ('for', 1234),\n",
       " ('not', 1212),\n",
       " ('as', 1179)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd2 = nltk.FreqDist(text2)\n",
    "fd2.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.4.3</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "The most frequent tokens are usually not very informative. We can do better by extracting all words from the text that are longer than seven characters and occur more than seven times. Change your function accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marianne', 566),\n",
       " ('Dashwood', 252),\n",
       " ('Jennings', 230),\n",
       " ('Willoughby', 215),\n",
       " ('Middleton', 102),\n",
       " ('therefore', 93),\n",
       " ('affection', 79),\n",
       " ('feelings', 73),\n",
       " ('together', 73),\n",
       " ('immediately', 72),\n",
       " ('something', 71),\n",
       " ('pleasure', 67),\n",
       " ('happiness', 66),\n",
       " ('acquaintance', 64),\n",
       " ('behaviour', 64),\n",
       " ('engagement', 62),\n",
       " ('attention', 58),\n",
       " ('continued', 56),\n",
       " ('situation', 54),\n",
       " ('returned', 52)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_seven = nltk.FreqDist(w for w in text2 if len(w) > 7 and fd2[w] > 7)\n",
    "fd_seven.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.5:</i> <br>\n",
    "</div> \n",
    "\n",
    "\n",
    "A stopword list contains high-frequency words like “the”, “to”, “and”, or “also” that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. NLTK provides a predefined list of stopwords: `nltk.corpus.stopwords.words('language')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.5.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look an the following sentence:<br><br>\n",
    "\n",
    "```Today I woke up early and went to the city to buy some things. Then I met a few of my friends and we ate something at a restaurant.``` <br><br>\n",
    "How many stopwords (according to the definition above) do you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "According to the definition, stopwords are words with little lexical content. In the given sentence, there are 14 stopwords:\n",
    "up, and, to, the, to, some, a, few, of, my, and, we, at, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.5.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "We now want to check your answer with the help NLTK. The code below is incomplete, change that so that when executed the found stopwords are printed. Compare the result with your answer from 8.4.1. Did you find all stopwords? <br>\n",
    "Hint: You do not have to use the segmenter if you want to solve the task differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-5dcf50c2861a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-5dcf50c2861a>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    stcStopwords = #todo\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stc = 'Today I woke up early and went to the city to buy some things. Then I met a few of my friends and we ate something at a restaurant'\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stcStopwords = #todo\n",
    "\n",
    "print(\"A total of \",len(stcStopwords),\" stopwords have been found: \", stcStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of  17  stopwords have been found:  ['I', 'up', 'and', 'to', 'the', 'to', 'some', 'Then', 'I', 'a', 'few', 'of', 'my', 'and', 'we', 'at', 'a']\n"
     ]
    }
   ],
   "source": [
    "stc = 'Today I woke up early and went to the city to buy some things. Then I met a few of my friends and we ate something at a restaurant'\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stcStopwords = [word for word in (word_tokenize(stc)) if word.lower() in stopwords]\n",
    "\n",
    "print(\"A total of \",len(stcStopwords),\" stopwords have been found: \", stcStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.5.3</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Change the function from 8.4.2 so that it finds all non-stopwords in the first 100 tokens from Moby Dick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of  36  stopwords have been found: \n",
      "\n",
      " ['by', 'by', 'a', 'to', 'a', 'The', 'in', 'and', 'I', 'him', 'now', 'He', 'was', 'his', 'and', 'with', 'a', 'with', 'all', 'the', 'of', 'all', 'the', 'of', 'the', 'He', 'to', 'his', 'it', 'him', 'of', 'his', 'While', 'you', 'in', 'to'] \n",
      "\n",
      " 64 tokens after stopword filtering: \n",
      "\n",
      " ['[', 'Moby', 'Dick', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'Late', 'Consumptive', 'Usher', 'Grammar', 'School', ')', 'pale', 'Usher', '--', 'threadbare', 'coat', ',', 'heart', ',', 'body', ',', 'brain', ';', 'see', '.', 'ever', 'dusting', 'old', 'lexicons', 'grammars', ',', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'gay', 'flags', 'known', 'nations', 'world', '.', 'loved', 'dust', 'old', 'grammars', ';', 'somehow', 'mildly', 'reminded', 'mortality', '.', '\"', 'take', 'hand', 'school', 'others', ',']\n"
     ]
    }
   ],
   "source": [
    "moby = nltk.corpus.gutenberg.words('melville-moby_dick.txt')[:100]\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "mobyStopwords = [word for word in moby if word.lower() in stopwords]\n",
    "mobyWithoutStopwords =  [word for word in moby if word.lower() not in stopwords]\n",
    "\n",
    "print(\"A total of \",len(mobyStopwords),\" stopwords have been found: \\n\\n\", mobyStopwords,\"\\n\\n\", len(moby)-len(mobyStopwords), \"tokens after stopword filtering: \\n\\n\", mobyWithoutStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-10-bd6ffce8d655>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-bd6ffce8d655>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    text1 = 'People are hungry before lunch and not hungry after lunch'\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def hasSameVocab(t1,t2):\n",
    "    #todo\n",
    "\n",
    "text1 = 'People are hungry before lunch and not hungry after lunch'\n",
    "text2 = 'People are not hungry before lunch and hungry after lunch'\n",
    "text3 = 'Humans are hungry before dinner and not hungry after dinner'\n",
    "\n",
    "print(hasSameVocab(text1, text2))\n",
    "print(hasSameVocab(text2, text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.5.4</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below. Enhance the function `hasSameVocab(tokens1,tokens2)`, so that it compares two texts. It should return true if the texts have the same vocabulary after removing the stopwords.<br> Hint: You can add more functions if you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def vocabWithoutStopwords(text):\n",
    "    return set(text) - set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def hasSameVocab(t1,t2):\n",
    "    return (vocabWithoutStopwords(t1) == vocabWithoutStopwords(t2))\n",
    "\n",
    "text1 = 'People are hungry before lunch and not hungry after lunch'\n",
    "text2 = 'People are not hungry before lunch and hungry after lunch'\n",
    "text3 = 'Humans are hungry before dinner and not hungry after dinner'\n",
    "\n",
    "print(hasSameVocab(text1, text2))#true\n",
    "print(hasSameVocab(text2, text3))#false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.6:</i> <br>\n",
    "</div> \n",
    "\n",
    "We want to find out which words in the following news article have the highest TF-IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_article = open(\"foreground_article.txt\", mode=\"r\", encoding=\"utf-8\").read()\n",
    "fg_article = word_tokenize(fg_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we have to prepare a background corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.6.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "We choose the Reuters corpus from the NLTK. What does the corpus contain? Why would you think this is a suitable background corpus? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "    \n",
    "The Reuters corpus contains a collection of 10,788 documents from the Reuters financial newswire service (This information can be obtained with ``reuters.readme()``).<br> It seems suitable because it is relatively large and like the article from which we want to extract keyphrases, it contains news texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.6.2</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "What does the following code compute? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788\n",
      "1720901\n"
     ]
    }
   ],
   "source": [
    "print(len(reuters.fileids()))\n",
    "print(len(reuters.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "Number of articles and total number of words in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.6.3</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "For each article, create a list of the tokens it contains and store these lists in one big list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_news = [reuters.words(fileid) for fileid in reuters.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...],\n",
       " ['CHINA', 'DAILY', 'SAYS', 'VERMIN', 'EAT', '7', '-', ...],\n",
       " ['JAPAN', 'TO', 'REVISE', 'LONG', '-', 'TERM', ...],\n",
       " ['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...],\n",
       " ['INDONESIA', 'SEES', 'CPO', 'PRICE', 'RISING', ...],\n",
       " ['AUSTRALIAN', 'FOREIGN', 'SHIP', 'BAN', 'ENDS', 'BUT', ...],\n",
       " ['INDONESIAN', 'COMMODITY', 'EXCHANGE', 'MAY', ...],\n",
       " ['SRI', 'LANKA', 'GETS', 'USDA', 'APPROVAL', 'FOR', ...],\n",
       " ['WESTERN', 'MINING', 'TO', 'OPEN', 'NEW', 'GOLD', ...],\n",
       " ['SUMITOMO', 'BANK', 'AIMS', 'AT', 'QUICK', 'RECOVERY', ...]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg_news[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.7.:</i> <br>\n",
    "</div> \n",
    "\n",
    "Now we compute the TF (term frequency) score for each word type in the forground corpus.<br> TF is defined here as the number of occurrences of the word type in the text divided by the total number of tokens in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.7.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "What does the following code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(foreground):\n",
    "    \n",
    "    tf_dict = dict.fromkeys(set(foreground), 0)\n",
    "    for word in foreground:\n",
    "        tf_dict[word] += 1\n",
    "        \n",
    "    return tf_dict \n",
    "   \n",
    "tf = computeTF(fg_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Americans': 1,\n",
       " 'stops': 1,\n",
       " 'Kennedy': 1,\n",
       " 'warning': 1,\n",
       " 'It': 1,\n",
       " 'Airport': 3,\n",
       " 'American': 2,\n",
       " 'rain': 3,\n",
       " 'completely': 1,\n",
       " 'weather': 7,\n",
       " 'no-fee': 1,\n",
       " 'where': 1,\n",
       " 'Lakes': 1,\n",
       " 'F.': 1,\n",
       " 'weather-delayed': 1,\n",
       " 'throughout': 1,\n",
       " '.': 16,\n",
       " 'staircase': 1,\n",
       " 'Mid-Atlantic': 1,\n",
       " 'affecting': 1,\n",
       " '``': 3,\n",
       " 'Sunday': 7,\n",
       " 'likely': 1,\n",
       " 'according': 2,\n",
       " 'offered': 2,\n",
       " 'Delta': 1,\n",
       " 'impact': 1,\n",
       " 'year': 1,\n",
       " 'Flight': 1,\n",
       " 'extremely': 1,\n",
       " 'Philadelphia': 1,\n",
       " 'million': 2,\n",
       " 'at': 1,\n",
       " 'get': 1,\n",
       " 'disruptions': 1,\n",
       " '56': 1,\n",
       " 'service': 2,\n",
       " '40': 1,\n",
       " 'From': 1,\n",
       " 'across': 1,\n",
       " 'Midwest': 1,\n",
       " 'no': 1,\n",
       " 'Pacific': 1,\n",
       " 'eight': 1,\n",
       " \"''\": 3,\n",
       " 'Travelers': 1,\n",
       " 'Tuesday': 1,\n",
       " 'snowfall': 1,\n",
       " 'Buffalo': 2,\n",
       " 'facing': 1,\n",
       " 'around': 1,\n",
       " 'California': 1,\n",
       " 'terminal': 1,\n",
       " 'upper-Midwest': 1,\n",
       " 'changes': 2,\n",
       " \"'s\": 3,\n",
       " 'Monday': 1,\n",
       " 'using': 1,\n",
       " 'York': 4,\n",
       " 'Valley': 1,\n",
       " 'treacherous': 1,\n",
       " 'that': 1,\n",
       " 'Aware': 1,\n",
       " 'roads': 1,\n",
       " 'parts': 3,\n",
       " 'charges': 1,\n",
       " 'Great': 1,\n",
       " 'hit': 1,\n",
       " 'issued': 1,\n",
       " 'wintry': 1,\n",
       " 'called': 1,\n",
       " 'England': 1,\n",
       " 'fully': 1,\n",
       " 'taxiing': 1,\n",
       " 'including': 1,\n",
       " 'than': 2,\n",
       " 'covered': 1,\n",
       " 'arriving': 1,\n",
       " 'forecast': 2,\n",
       " 'returning': 1,\n",
       " 'Dakotas': 2,\n",
       " 'also': 2,\n",
       " 'Minnesota': 1,\n",
       " 'possible': 1,\n",
       " 'inches': 1,\n",
       " 'airports': 2,\n",
       " 'tracking': 1,\n",
       " 'more': 3,\n",
       " 'morning': 2,\n",
       " 'drifts': 1,\n",
       " 'carrier': 1,\n",
       " 'in': 14,\n",
       " '2.7': 1,\n",
       " 'from': 4,\n",
       " 'Michigan': 2,\n",
       " 'stuck': 2,\n",
       " 'near': 1,\n",
       " 'LaGuardia': 2,\n",
       " 'southern': 1,\n",
       " 'Conditions': 1,\n",
       " 'hubs': 2,\n",
       " 'News': 1,\n",
       " 'National': 2,\n",
       " 'impassable': 2,\n",
       " 'flights': 2,\n",
       " 'Passengers': 1,\n",
       " 'Pennsylvania': 1,\n",
       " 'Duluth': 1,\n",
       " 'country': 2,\n",
       " 'on': 1,\n",
       " 'grass': 1,\n",
       " 'John': 1,\n",
       " 'and': 17,\n",
       " 'vehicles': 1,\n",
       " 'warnings': 1,\n",
       " 'Hundreds': 1,\n",
       " 'be': 2,\n",
       " 'snow': 5,\n",
       " 'effect': 1,\n",
       " 'NBC': 1,\n",
       " 'The': 2,\n",
       " 'evening': 1,\n",
       " 'numerous': 1,\n",
       " 'showers': 1,\n",
       " 'the': 26,\n",
       " 'an': 2,\n",
       " 'Saturday': 1,\n",
       " 'Ice': 1,\n",
       " 'foot': 1,\n",
       " 'woes': 1,\n",
       " 'travelers': 2,\n",
       " 'once': 1,\n",
       " 'will': 1,\n",
       " '423': 1,\n",
       " 'could': 2,\n",
       " 'travel': 4,\n",
       " 'estimate': 1,\n",
       " 'Airlines': 2,\n",
       " 'causing': 1,\n",
       " 'upper': 2,\n",
       " 'Service': 2,\n",
       " 'Weather': 2,\n",
       " 'statement': 2,\n",
       " 'after': 2,\n",
       " 'days': 1,\n",
       " 'forecasters': 1,\n",
       " 'is': 2,\n",
       " 'destinations': 1,\n",
       " 'are': 6,\n",
       " 'western': 1,\n",
       " 'again': 1,\n",
       " 'national': 1,\n",
       " 'Southern': 1,\n",
       " 'potential': 1,\n",
       " 'home': 1,\n",
       " 'of': 15,\n",
       " 'most': 1,\n",
       " 'severe': 2,\n",
       " 'very': 1,\n",
       " 'officials': 1,\n",
       " 'eastern': 1,\n",
       " 'North': 1,\n",
       " '22': 1,\n",
       " 'In': 1,\n",
       " 'as': 1,\n",
       " 'customers': 2,\n",
       " 'hour': 1,\n",
       " 'its': 4,\n",
       " 'use': 1,\n",
       " 'millions': 1,\n",
       " 'canceled': 2,\n",
       " 'Maryland': 1,\n",
       " 'by': 3,\n",
       " 'Delays': 2,\n",
       " 'Roads': 1,\n",
       " 'through': 1,\n",
       " 'some': 3,\n",
       " 'it': 1,\n",
       " 'got': 2,\n",
       " 'airline': 1,\n",
       " 'freezing': 2,\n",
       " 'extra': 1,\n",
       " 'heavy': 2,\n",
       " 'New': 5,\n",
       " 'Mississippi': 2,\n",
       " 'bused': 1,\n",
       " 'with': 1,\n",
       " ',': 24,\n",
       " 'plane': 2,\n",
       " 'alert': 2,\n",
       " 'when': 1,\n",
       " 'Thanksgiving': 2,\n",
       " 'Upstate': 1,\n",
       " 'affected': 2,\n",
       " 'thunderstorms': 1,\n",
       " 'Northern': 1,\n",
       " 'flight': 4,\n",
       " 'underway': 1,\n",
       " 'were': 6,\n",
       " 'expected': 1,\n",
       " 'select': 1,\n",
       " 'storm': 1,\n",
       " 'for': 7,\n",
       " 'up': 1,\n",
       " 'Tennessee': 1,\n",
       " 'delays': 2,\n",
       " 'northern': 1,\n",
       " 'slid': 2,\n",
       " 'busiest': 1,\n",
       " 'off': 2,\n",
       " 'exited': 1,\n",
       " 'pre-winter': 1,\n",
       " 'said': 7,\n",
       " '16': 2,\n",
       " 'a': 7,\n",
       " 'to': 12,\n",
       " 'valleys': 1,\n",
       " 'Northeast': 4,\n",
       " 'Northwest': 1,\n",
       " 'runway': 2,\n",
       " 'one': 1,\n",
       " 'central': 1,\n",
       " 'International': 2,\n",
       " 'ditches': 1,\n",
       " 'website': 1,\n",
       " 'scheduled': 1,\n",
       " 'cause': 1,\n",
       " 'which': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "    \n",
    "The frequency of each word in the foreground corpus is computed and stored in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.7.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Change the function so that  the returned dictionary does not contatin the raw frequency but the frequency divided by the total number of tokens in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(foreground):\n",
    "    \n",
    "    tf_dict = dict.fromkeys(set(foreground), 0)\n",
    "    for word in foreground:\n",
    "        tf_dict[word] += 1\n",
    "        \n",
    "    for word, count in tf_dict.items():\n",
    "        tf_dict[word] = count / float(len(foreground))\n",
    "        \n",
    "    return tf_dict\n",
    "\n",
    "tf = computeTF(fg_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Americans': 0.0020833333333333333,\n",
       " 'stops': 0.0020833333333333333,\n",
       " 'Kennedy': 0.0020833333333333333,\n",
       " 'warning': 0.0020833333333333333,\n",
       " 'It': 0.0020833333333333333,\n",
       " 'Airport': 0.00625,\n",
       " 'American': 0.004166666666666667,\n",
       " 'rain': 0.00625,\n",
       " 'completely': 0.0020833333333333333,\n",
       " 'weather': 0.014583333333333334,\n",
       " 'no-fee': 0.0020833333333333333,\n",
       " 'where': 0.0020833333333333333,\n",
       " 'Lakes': 0.0020833333333333333,\n",
       " 'F.': 0.0020833333333333333,\n",
       " 'weather-delayed': 0.0020833333333333333,\n",
       " 'throughout': 0.0020833333333333333,\n",
       " '.': 0.03333333333333333,\n",
       " 'staircase': 0.0020833333333333333,\n",
       " 'Mid-Atlantic': 0.0020833333333333333,\n",
       " 'affecting': 0.0020833333333333333,\n",
       " '``': 0.00625,\n",
       " 'Sunday': 0.014583333333333334,\n",
       " 'likely': 0.0020833333333333333,\n",
       " 'according': 0.004166666666666667,\n",
       " 'offered': 0.004166666666666667,\n",
       " 'Delta': 0.0020833333333333333,\n",
       " 'impact': 0.0020833333333333333,\n",
       " 'year': 0.0020833333333333333,\n",
       " 'Flight': 0.0020833333333333333,\n",
       " 'extremely': 0.0020833333333333333,\n",
       " 'Philadelphia': 0.0020833333333333333,\n",
       " 'million': 0.004166666666666667,\n",
       " 'at': 0.0020833333333333333,\n",
       " 'get': 0.0020833333333333333,\n",
       " 'disruptions': 0.0020833333333333333,\n",
       " '56': 0.0020833333333333333,\n",
       " 'service': 0.004166666666666667,\n",
       " '40': 0.0020833333333333333,\n",
       " 'From': 0.0020833333333333333,\n",
       " 'across': 0.0020833333333333333,\n",
       " 'Midwest': 0.0020833333333333333,\n",
       " 'no': 0.0020833333333333333,\n",
       " 'Pacific': 0.0020833333333333333,\n",
       " 'eight': 0.0020833333333333333,\n",
       " \"''\": 0.00625,\n",
       " 'Travelers': 0.0020833333333333333,\n",
       " 'Tuesday': 0.0020833333333333333,\n",
       " 'snowfall': 0.0020833333333333333,\n",
       " 'Buffalo': 0.004166666666666667,\n",
       " 'facing': 0.0020833333333333333,\n",
       " 'around': 0.0020833333333333333,\n",
       " 'California': 0.0020833333333333333,\n",
       " 'terminal': 0.0020833333333333333,\n",
       " 'upper-Midwest': 0.0020833333333333333,\n",
       " 'changes': 0.004166666666666667,\n",
       " \"'s\": 0.00625,\n",
       " 'Monday': 0.0020833333333333333,\n",
       " 'using': 0.0020833333333333333,\n",
       " 'York': 0.008333333333333333,\n",
       " 'Valley': 0.0020833333333333333,\n",
       " 'treacherous': 0.0020833333333333333,\n",
       " 'that': 0.0020833333333333333,\n",
       " 'Aware': 0.0020833333333333333,\n",
       " 'roads': 0.0020833333333333333,\n",
       " 'parts': 0.00625,\n",
       " 'charges': 0.0020833333333333333,\n",
       " 'Great': 0.0020833333333333333,\n",
       " 'hit': 0.0020833333333333333,\n",
       " 'issued': 0.0020833333333333333,\n",
       " 'wintry': 0.0020833333333333333,\n",
       " 'called': 0.0020833333333333333,\n",
       " 'England': 0.0020833333333333333,\n",
       " 'fully': 0.0020833333333333333,\n",
       " 'taxiing': 0.0020833333333333333,\n",
       " 'including': 0.0020833333333333333,\n",
       " 'than': 0.004166666666666667,\n",
       " 'covered': 0.0020833333333333333,\n",
       " 'arriving': 0.0020833333333333333,\n",
       " 'forecast': 0.004166666666666667,\n",
       " 'returning': 0.0020833333333333333,\n",
       " 'Dakotas': 0.004166666666666667,\n",
       " 'also': 0.004166666666666667,\n",
       " 'Minnesota': 0.0020833333333333333,\n",
       " 'possible': 0.0020833333333333333,\n",
       " 'inches': 0.0020833333333333333,\n",
       " 'airports': 0.004166666666666667,\n",
       " 'tracking': 0.0020833333333333333,\n",
       " 'more': 0.00625,\n",
       " 'morning': 0.004166666666666667,\n",
       " 'drifts': 0.0020833333333333333,\n",
       " 'carrier': 0.0020833333333333333,\n",
       " 'in': 0.029166666666666667,\n",
       " '2.7': 0.0020833333333333333,\n",
       " 'from': 0.008333333333333333,\n",
       " 'Michigan': 0.004166666666666667,\n",
       " 'stuck': 0.004166666666666667,\n",
       " 'near': 0.0020833333333333333,\n",
       " 'LaGuardia': 0.004166666666666667,\n",
       " 'southern': 0.0020833333333333333,\n",
       " 'Conditions': 0.0020833333333333333,\n",
       " 'hubs': 0.004166666666666667,\n",
       " 'News': 0.0020833333333333333,\n",
       " 'National': 0.004166666666666667,\n",
       " 'impassable': 0.004166666666666667,\n",
       " 'flights': 0.004166666666666667,\n",
       " 'Passengers': 0.0020833333333333333,\n",
       " 'Pennsylvania': 0.0020833333333333333,\n",
       " 'Duluth': 0.0020833333333333333,\n",
       " 'country': 0.004166666666666667,\n",
       " 'on': 0.0020833333333333333,\n",
       " 'grass': 0.0020833333333333333,\n",
       " 'John': 0.0020833333333333333,\n",
       " 'and': 0.035416666666666666,\n",
       " 'vehicles': 0.0020833333333333333,\n",
       " 'warnings': 0.0020833333333333333,\n",
       " 'Hundreds': 0.0020833333333333333,\n",
       " 'be': 0.004166666666666667,\n",
       " 'snow': 0.010416666666666666,\n",
       " 'effect': 0.0020833333333333333,\n",
       " 'NBC': 0.0020833333333333333,\n",
       " 'The': 0.004166666666666667,\n",
       " 'evening': 0.0020833333333333333,\n",
       " 'numerous': 0.0020833333333333333,\n",
       " 'showers': 0.0020833333333333333,\n",
       " 'the': 0.05416666666666667,\n",
       " 'an': 0.004166666666666667,\n",
       " 'Saturday': 0.0020833333333333333,\n",
       " 'Ice': 0.0020833333333333333,\n",
       " 'foot': 0.0020833333333333333,\n",
       " 'woes': 0.0020833333333333333,\n",
       " 'travelers': 0.004166666666666667,\n",
       " 'once': 0.0020833333333333333,\n",
       " 'will': 0.0020833333333333333,\n",
       " '423': 0.0020833333333333333,\n",
       " 'could': 0.004166666666666667,\n",
       " 'travel': 0.008333333333333333,\n",
       " 'estimate': 0.0020833333333333333,\n",
       " 'Airlines': 0.004166666666666667,\n",
       " 'causing': 0.0020833333333333333,\n",
       " 'upper': 0.004166666666666667,\n",
       " 'Service': 0.004166666666666667,\n",
       " 'Weather': 0.004166666666666667,\n",
       " 'statement': 0.004166666666666667,\n",
       " 'after': 0.004166666666666667,\n",
       " 'days': 0.0020833333333333333,\n",
       " 'forecasters': 0.0020833333333333333,\n",
       " 'is': 0.004166666666666667,\n",
       " 'destinations': 0.0020833333333333333,\n",
       " 'are': 0.0125,\n",
       " 'western': 0.0020833333333333333,\n",
       " 'again': 0.0020833333333333333,\n",
       " 'national': 0.0020833333333333333,\n",
       " 'Southern': 0.0020833333333333333,\n",
       " 'potential': 0.0020833333333333333,\n",
       " 'home': 0.0020833333333333333,\n",
       " 'of': 0.03125,\n",
       " 'most': 0.0020833333333333333,\n",
       " 'severe': 0.004166666666666667,\n",
       " 'very': 0.0020833333333333333,\n",
       " 'officials': 0.0020833333333333333,\n",
       " 'eastern': 0.0020833333333333333,\n",
       " 'North': 0.0020833333333333333,\n",
       " '22': 0.0020833333333333333,\n",
       " 'In': 0.0020833333333333333,\n",
       " 'as': 0.0020833333333333333,\n",
       " 'customers': 0.004166666666666667,\n",
       " 'hour': 0.0020833333333333333,\n",
       " 'its': 0.008333333333333333,\n",
       " 'use': 0.0020833333333333333,\n",
       " 'millions': 0.0020833333333333333,\n",
       " 'canceled': 0.004166666666666667,\n",
       " 'Maryland': 0.0020833333333333333,\n",
       " 'by': 0.00625,\n",
       " 'Delays': 0.004166666666666667,\n",
       " 'Roads': 0.0020833333333333333,\n",
       " 'through': 0.0020833333333333333,\n",
       " 'some': 0.00625,\n",
       " 'it': 0.0020833333333333333,\n",
       " 'got': 0.004166666666666667,\n",
       " 'airline': 0.0020833333333333333,\n",
       " 'freezing': 0.004166666666666667,\n",
       " 'extra': 0.0020833333333333333,\n",
       " 'heavy': 0.004166666666666667,\n",
       " 'New': 0.010416666666666666,\n",
       " 'Mississippi': 0.004166666666666667,\n",
       " 'bused': 0.0020833333333333333,\n",
       " 'with': 0.0020833333333333333,\n",
       " ',': 0.05,\n",
       " 'plane': 0.004166666666666667,\n",
       " 'alert': 0.004166666666666667,\n",
       " 'when': 0.0020833333333333333,\n",
       " 'Thanksgiving': 0.004166666666666667,\n",
       " 'Upstate': 0.0020833333333333333,\n",
       " 'affected': 0.004166666666666667,\n",
       " 'thunderstorms': 0.0020833333333333333,\n",
       " 'Northern': 0.0020833333333333333,\n",
       " 'flight': 0.008333333333333333,\n",
       " 'underway': 0.0020833333333333333,\n",
       " 'were': 0.0125,\n",
       " 'expected': 0.0020833333333333333,\n",
       " 'select': 0.0020833333333333333,\n",
       " 'storm': 0.0020833333333333333,\n",
       " 'for': 0.014583333333333334,\n",
       " 'up': 0.0020833333333333333,\n",
       " 'Tennessee': 0.0020833333333333333,\n",
       " 'delays': 0.004166666666666667,\n",
       " 'northern': 0.0020833333333333333,\n",
       " 'slid': 0.004166666666666667,\n",
       " 'busiest': 0.0020833333333333333,\n",
       " 'off': 0.004166666666666667,\n",
       " 'exited': 0.0020833333333333333,\n",
       " 'pre-winter': 0.0020833333333333333,\n",
       " 'said': 0.014583333333333334,\n",
       " '16': 0.004166666666666667,\n",
       " 'a': 0.014583333333333334,\n",
       " 'to': 0.025,\n",
       " 'valleys': 0.0020833333333333333,\n",
       " 'Northeast': 0.008333333333333333,\n",
       " 'Northwest': 0.0020833333333333333,\n",
       " 'runway': 0.004166666666666667,\n",
       " 'one': 0.0020833333333333333,\n",
       " 'central': 0.0020833333333333333,\n",
       " 'International': 0.004166666666666667,\n",
       " 'ditches': 0.0020833333333333333,\n",
       " 'website': 0.0020833333333333333,\n",
       " 'scheduled': 0.0020833333333333333,\n",
       " 'cause': 0.0020833333333333333,\n",
       " 'which': 0.0020833333333333333}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.8.:</i> <br>\n",
    "</div> \n",
    "\n",
    "The IDF (inverse document frequency) for word w is defined as log(number of documents in the background corpus / number of documents containing word w)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.8.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "What does the following code return? What would change if we changed the line ``for word in set(document)`` to ``for word in document``?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(foreground, background):\n",
    "    bg_dict = {}\n",
    "    for document in background:\n",
    "        for word in set(document):\n",
    "            if word not in bg_dict:\n",
    "                bg_dict[word] = 1\n",
    "            else:\n",
    "                bg_dict[word] += 1\n",
    "    \n",
    "    return bg_dict\n",
    "\n",
    "news_idf = computeIDF(fg_article, bg_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "    \n",
    "The returned dictionary contains for each word in the background corpus in how many documents it appears. If we omit `set()` in the line `for word in set(document):`, we would not compute in how many documents a word appears (each document counting only once) but how many times it appears in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.8.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Enhance the function so that it returns a dictionary with the IDF value of each word in the foreground corpus. Take care that divisions by zero cannot occur!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(foreground, background): \n",
    "    bg_dict = {}\n",
    "    for document in background:\n",
    "        for word in set(document):\n",
    "            if word not in bg_dict:\n",
    "                bg_dict[word] = 1\n",
    "            else:\n",
    "                bg_dict[word] += 1\n",
    "    \n",
    "    N = len(background) #total number of documents\n",
    "    \n",
    "    idfDict = dict.fromkeys(foreground, 0)\n",
    "    for word in idfDict.keys():\n",
    "        if word in bg_dict: #in this solution, words which appear in no document receive an idf value of 0\n",
    "            idfDict[word] = math.log(N / float(bg_dict[word]))\n",
    "    return idfDict\n",
    "\n",
    "\n",
    "news_idf = computeIDF(fg_article, bg_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.9.:</i> <br>\n",
    "</div> \n",
    "\n",
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.9.1</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Now compute the TF-IDF value of each word in the foreground corpus by multiplying the TF and the IDF values. Print the 10 words with the highest TF-IDF value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tf, idf):\n",
    "    tfidf = {}\n",
    "    #To Do\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday 0.07927561370384778\n",
      "snow 0.07001292007081336\n",
      "weather 0.06674295079298127\n",
      "Northeast 0.060056234521498206\n",
      "flight 0.05907470922436168\n",
      "travel 0.05428000801683199\n",
      "Airport 0.046840188843947285\n",
      "Buffalo 0.03869245701774841\n",
      "travelers 0.03580434376541531\n",
      "canceled 0.03580434376541531\n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tf, idf):\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val * idf[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidf = computeTFIDF(tf, news_idf)\n",
    "\n",
    "#highest value\n",
    "for word in sorted(tfidf, key=tfidf.get, reverse=True)[:10]:\n",
    "    print(word, tfidf[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.9.2</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Now print the 10 words with the lowest TF-IDF value (that is not 0). How many of them are stopwords?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0.0018878994417108916\n",
      "on 0.0021133034439685676\n",
      "at 0.0025001815595635836\n",
      "with 0.00252988861385513\n",
      "year 0.0027412391188920654\n",
      "that 0.002779013282505364\n",
      "will 0.0028078002772809005\n",
      "which 0.0031259766410937427\n",
      "The 0.0032325821591312857\n",
      "as 0.0032665598589589687\n",
      "Overlap with stopwords: {'the', 'with', 'on', 'will', 'at', 'it', 'that', 'as', 'which'}\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#delete entries with value 0 (=those which were not found in the background corpus at all)\n",
    "tfidf = {key:value for (key,value) in tfidf.items() if value > 0}\n",
    "\n",
    "lowest_tfidf = set([])\n",
    "\n",
    "for word in sorted(tfidf, key=tfidf.get)[:10]:\n",
    "    print(word, tfidf[word])\n",
    "    lowest_tfidf.add(word.lower())\n",
    "\n",
    "#overlap with stopwords\n",
    "print(\"Overlap with stopwords:\", lowest_tfidf.intersection(stopwords))\n",
    "\n",
    "print(len(lowest_tfidf.intersection(stopwords)))\n",
    "\n",
    "#The only non-stopword in the list of words with the lowest TF-IDF scores is \"year\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 8.10.:</i> <br>\n",
    "</div> \n",
    "\n",
    "Now try what happens if you use the move_reviews corpus from the NLTK instead of the reuters corpus as background corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.10.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Which components will change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "    \n",
    "Only the IDF and the final TF-IDF scores will change. The TF score remains the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.10.2</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Are the 10 words with the highest tf-idf values the same? Compute the new tf-idf scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare new background corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_movies = [movie_reviews.words(fileid) for fileid in movie_reviews.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute new IDF values (TF values remain the same!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_idf = computeIDF(fg_article, bg_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute new TF-IDF scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_movies = computeTFIDF(tf, movie_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 10 words with the highest scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather 0.07726712826215887\n",
      "snow 0.04184774501131221\n",
      "`` 0.0388413006151387\n",
      "'' 0.03377298676378664\n",
      "flight 0.033712953317105575\n",
      "forecast 0.02878231366242557\n",
      "runway 0.02878231366242557\n",
      "said 0.028060501251909595\n",
      "travel 0.028056632957873525\n",
      "canceled 0.027092875711974884\n"
     ]
    }
   ],
   "source": [
    "for word in sorted(tfidf_movies, key=tfidf_movies.get, reverse=True)[:10]:\n",
    "    print(word, tfidf_movies[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">8.1.</i> :::10 Homework points:::\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design an algorithm to find the \"statistically improbable phrases\" (SIP) of a document collection:<br><br>\n",
    "https://web.archive.org/web/20160425062531/http://www.amazon.com/gp/search-inside/sipshelp.html <br><br>\n",
    "The main idea behind SIPs is that if a phrase occurs much more often in a document than this is statistically expected (hence the name “statistically improbable phrases”), it might be a good indicator of what a document is about.<br><br> For example, we may look at a corpus with 1,000,000 words and see that the word “car” appears 20 times. If now the word “car” also appears 20 times in a document with 1000 words, this seems improbable.<br><br>\n",
    "To formalize the counting of words a bit, we may use so called contingency tables. They are a structured way to display how often a word occurs in certain documents. <br> As we are only interested in the counts in the current document as well as the counts in the overall collection, we will use a 2x2 table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|-|Document<br>(Foreground Corpus)|Other Corpus<br>(Background Corpus)|-|\n",
    "|-|-|-|-|\n",
    "|word|A|B|-\n",
    "|all other words|C-A|D-B|-\n",
    "|-|C|D|N = C + D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now measure how “improbable” is a certain phrase in a document (also called foreground corpus) w.r.t. some corpus (called background corpus) using “Log-Likelyhood” (LL) as defined by (Rayson & Garside, 2000). <br><br>In terms of the contingency table, it is defined as:<br><br>\n",
    "\n",
    "$ LL = 2*(A*log_2(\\frac{A}{E_1}) + B*log_2(\\frac{B}{E_2}))$ <br><br> where <br><br>\n",
    "$E_1 = \\frac{C*(A+B)}{N}$ and $E_2 = \\frac{D*(A+B)}{N}$ <br><br> For simplifications, you may limit phrases to bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.1.</i>\n",
    "</div>\n",
    "\n",
    "Select a suitable corpus from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.2.</i>\n",
    "</div>\n",
    "\n",
    "Initialize two frequency distributions of bigrams. <br>One from a text of your choice (`fdist_fg`) and one of the\n",
    "corpus selected in 8.1.1. (`fdist_bg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley's sister, but they hadn't met for several years; in fact, Mrs. Dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn't want Dudley mixing with a child like that. When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair. None of them noticed a large, tawny owl flutter past the window. At half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. Little tyke, chortled Mr. Dursley as he left the house. He got into his car and backed out of number four's drive. It was on the corner of the street that he noticed the first sign of something peculiar — a cat reading a map. For a second, Mr. Dursley didn't realize what he had seen — then he jerked his head around to look again. There was a tabby cat standing on the corner of Privet Drive, but there wasn't a map in sight. What could he have been thinking of? It must have been a trick of the light. Mr. Dursley blinked and stared at the cat. It stared back. As Mr. Dursley drove around the corner and up the road, he watched the cat in his mirror. It was now reading the sign that said Privet Drive — no, looking at the sign; cats couldn't read maps or signs. Mr. Dursley gave himself a little shake and put the cat out of his mind. As he drove toward town he thought of nothing except a large order of drills he was hoping to get that day. But on the edge of town, drills were driven out of his mind by something else. As he sat in the usual morning traffic jam, he couldn't help noticing that there seemed to be a lot of strangely dressed people about. People in cloaks. Mr. Dursley couldn't bear people who dressed in funny clothes — the getups you saw on young people! He supposed this was some stupid new fashion. He drummed his fingers on the steering wheel and his eyes fell on a huddle of these weirdos standing quite close by. They were whispering excitedly together. Mr. Dursley was enraged to see that a couple of them weren't young at all; why, that man had to be older than he was, and wearing an emerald-green cloak! The nerve of him! But then it struck Mr. Dursley that this was probably some silly stunt — these people were obviously collecting for something... yes, that would be it. The traffic moved on and a few minutes later, Mr. Dursley arrived in the Grunnings parking lot, his mind back on drills. Mr. Dursley always sat with his back to the window in his office on the ninth floor. If he hadn't, he might have found it harder to concentrate on drills that morning. He didn't see the owls swoop ing past in broad daylight, though people down in the street did; they pointed and gazed open- mouthed as owl after owl sped overhead. Most of them had never seen an owl even at nighttime. Mr. Dursley, however, had a perfectly normal, owl-free morning. He yelled at five different people. He made several important telephone calls and shouted a bit more. He was in a very good mood until lunchtime, when he thought he'd stretch his legs and walk across the road to buy himself a bun from the bakery. He'd forgotten all about the people in cloaks until he passed a group of them next to the baker's. He eyed them angrily as he passed. He didn't know why, but they made him uneasy. This bunch were whispering excitedly, too, and he couldn't see a single collecting tin. It was on his way back past them, clutching a large doughnut in a bag, that he caught a few words of what they were saying.\"\n",
    "fg_article = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_fg = nltk.FreqDist([bigram for bigram in nltk.bigrams(fg_article)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_list = []\n",
    "    \n",
    "for document in [gutenberg.words(fileid) for fileid in gutenberg.fileids()]:\n",
    "    for bigram in nltk.bigrams(document):\n",
    "        bg_list.append(bigram)\n",
    "\n",
    "fdist_bg = nltk.FreqDist(bg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('Mr.', 'Dursley'): 14, ('.', 'He'): 10, ('.', 'Mr.'): 8, ('on', 'the'): 8, ('Mrs.', 'Dursley'): 7, ('did', \"n't\"): 7, ('.', 'The'): 5, (',', 'but'): 5, (',', 'and'): 5, ('to', 'be'): 4, ...})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({(',', 'and'): 41294, ('of', 'the'): 18912, ('in', 'the'): 9793, (\"'\", 's'): 9781, (';', 'and'): 7559, ('and', 'the'): 6432, ('the', 'LORD'): 5964, (',', 'the'): 5957, (',', 'I'): 5677, (',', 'that'): 5352, ...})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_bg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.3.</i>\n",
    "</div>\n",
    "\n",
    "Using the frequency distributions, determine how to get all the required counts to compute the log likelihood\n",
    "scores and write a function ``compute_LL(phrase,fdist_fg,fdist_bg).``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_LL(phrase, fdist_fg, fdist_bg):\n",
    "    A = fdist_fg[phrase]\n",
    "    B = fdist_bg[phrase]\n",
    "    C = fdist_fg.N()\n",
    "    D = fdist_bg.N()\n",
    "    N = C + D\n",
    "    E1 = (C*(A+B))/N\n",
    "    E2 = (D*(A+B))/N\n",
    "    if A!=0 and B!=0:\n",
    "        LL = 2*(A*math.log2(A/float(E1)) + B*math.log2(B/float(E2)))\n",
    "    elif A!=0 and B==0:\n",
    "        LL = 2*(A*math.log2(A/float(E1)) + 0)\n",
    "    elif A==0 and B!=0:\n",
    "        LL = 2*(0 + B*math.log2(B/float(E2)))\n",
    "    else:\n",
    "        LL=0\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">8.1.4.</i>\n",
    "</div>\n",
    "\n",
    "Given a text, calculate the LL score for each bigram and output the 10 most “improbable” phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mr.', 'Dursley') 315.032005326403\n",
      "('.', 'Mr.') 180.01828875794456\n",
      "('Mrs.', 'Dursley') 157.5160026632015\n",
      "('did', \"n't\") 157.5160026632015\n",
      "('The', 'Dursleys') 90.00914437897228\n",
      "('the', 'Potters') 90.00914437897228\n",
      "('could', \"n't\") 90.00914437897228\n",
      "('and', 'Mrs.') 67.50685828422921\n",
      "('Privet', 'Drive') 67.50685828422921\n",
      "('Dursley', 'was') 67.50685828422921\n"
     ]
    }
   ],
   "source": [
    "bigrams_fg = fdist_fg.keys()\n",
    "\n",
    "ll_dict_fg = dict.fromkeys(bigrams_fg, 0)\n",
    "\n",
    "for bigram in bigrams_fg:\n",
    "    ll = compute_LL(bigram, fdist_fg, fdist_bg)\n",
    "    ll_dict_fg[bigram] = ll\n",
    "    \n",
    "for bigram in sorted(ll_dict_fg, key=ll_dict_fg.get, reverse=True)[:10]:\n",
    "    print(bigram, ll_dict_fg[bigram])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
